import numpy as np
import keras
from keras import backend as K
from keras.layers import GaussianNoise,Dropout
from keras.models import Sequential
from keras.layers.core import Dense, Flatten
from keras.layers import SeparableConv2D, MaxPooling2D, Activation, Concatenate, AveragePooling2D, Dropout, GlobalAveragePooling2D
from keras.optimizers import Adam, SGD, Adadelta, RMSprop, Nadam
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import *
from keras.layers import Input
from keras.models import Model
import matplotlib.pyplot as plt
import cv2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.image import save_img
import scipy.misc

path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_train'
v_path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_valid'
t_path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_test'

batches = ImageDataGenerator().flow_from_directory(path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=32)
v_batches = ImageDataGenerator().flow_from_directory(v_path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=32)
t_batches = ImageDataGenerator().flow_from_directory(t_path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=250)
t_imgs,t_labels = next(t_batches)

img_input = Input(shape=(224,224,3))
#image = GaussianNoise(0.01)(img_input)
image = BatchNormalization()(img_input,training=True)
image = SeparableConv2D(16, (1,1), strides=(2,2), activation='relu')(image)
image = SeparableConv2D(16, (5,5), strides=(2,2), activation='relu')(image)
image = MaxPooling2D(pool_size=(3,3), strides=(2,2))(image)
image = SeparableConv2D(32, (3,3), strides=(2,2), activation='relu')(image)
image = BatchNormalization()(image,training=True)


"""

tower_3a1 = Conv2D(22, (1,1), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(image)

tower_3a2 = Conv2D(32, (1,1), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(image)
tower_3a2 = Conv2D(43, (3,3), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(tower_3a2)

tower_3a3 = Conv2D(6, (1,1), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(image)
tower_3a3 = Conv2D(11, (5,1), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(tower_3a3)
tower_3a3 = Conv2D(11, (1,5), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(tower_3a3)

tower_3a4 = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same',)(image)
tower_3a4 = Conv2D(11, (1,1), activation='relu', padding='same',kernel_regularizer=regularizers.l2(0.0035))(tower_3a4)

t_3a = keras.layers.concatenate([tower_3a1,tower_3a2,tower_3a3,tower_3a4], axis = 3)
t_3a = BatchNormalization()(t_3a)

"""

tower_1a1 = SeparableConv2D(24, (1,1), activation='relu', padding='same')(image)
tower_1a1 = SeparableConv2D(24, (3,3), activation='relu')(tower_1a1)

tower_1a2 = SeparableConv2D(32, (1,1), activation='relu', padding='same')(image)
tower_1a2 = SeparableConv2D(32, (7,1), activation='relu', padding='same')(tower_1a2)
tower_1a2 = SeparableConv2D(40, (1,7), activation='relu', padding='same')(tower_1a2)
tower_1a2 = SeparableConv2D(40, (3,3), activation='relu')(tower_1a2)

t_1 = keras.layers.concatenate([tower_1a1,tower_1a2], axis = 3)
t_1 = Dropout(0.2)(t_1)
t_1 = BatchNormalization()(t_1,training=True)



tower_2a1 = SeparableConv2D(12, (1,1), activation='relu', padding='same')(t_1)

tower_2a2 = SeparableConv2D(8, (1,1), activation='relu', padding='same')(t_1)
tower_2a2 = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_2a2)

tower_2a3 = SeparableConv2D(8, (1,1), activation='relu', padding='same')(t_1)
tower_2a3 = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_2a3)
tower_2a3 = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_2a3)

tower_2a4 = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_1)
tower_2a4 = SeparableConv2D(12, (1,1), activation='relu', padding='same')(tower_2a4)

t_2 = keras.layers.concatenate([tower_2a1,tower_2a2,tower_2a3,tower_2a4], axis = 3)
t_2 = Dropout(0.2)(t_2)
t_2 = BatchNormalization()(t_2,training=True)



tower_3a1 = SeparableConv2D(48, (3,3), activation='relu', strides=(2,2))(t_2)

tower_3a2 = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_2)
tower_3a2 = SeparableConv2D(28, (3,3), activation='relu', padding='same')(tower_3a2)
tower_3a2 = SeparableConv2D(32, (3,3), activation='relu' , strides=(2,2))(tower_3a2)

tower_3a3 = MaxPooling2D(pool_size=(3,3), strides=(2,2))(t_2)

t_3 = keras.layers.concatenate([tower_3a1,tower_3a2,tower_3a3], axis = 3)
t_3 = Dropout(0.2)(t_3)
t_3 = BatchNormalization()(t_3,training=True)



tower_4a = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_3)

tower_4b = SeparableConv2D(46, (1,1), activation='relu', padding='same')(t_3)
tower_4b = SeparableConv2D(56, (1,7), activation='relu', padding='same')(tower_4b)
tower_4b = SeparableConv2D(64, (7,1), activation='relu', padding='same')(tower_4b)
tower_4b1 = SeparableConv2D(32, (1,7), activation='relu', padding='same')(tower_4b)
tower_4b2 = SeparableConv2D(32, (7,1), activation='relu', padding='same')(tower_4b)

tower_4c = SeparableConv2D(48, (1,1), activation='relu', padding='same')(t_3)
tower_4c1 = SeparableConv2D(32, (1,7), activation='relu', padding='same')(tower_4c)
tower_4c2 = SeparableConv2D(32, (7,1), activation='relu', padding='same')(tower_4c)

tower_4d = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_3)
tower_4d = SeparableConv2D(32, (1,1), activation='relu', padding='same')(tower_4d)

t_4 = keras.layers.concatenate([tower_4a,tower_4b1,tower_4b2,tower_4c1,tower_4c2,tower_4d], axis = 3)
t_4 = Dropout(0.2)(t_4)
t_4 = BatchNormalization()(t_4,training=True)


tower_5a1 = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_4)
tower_5a1 = SeparableConv2D(32, (1,7), activation='relu', padding='same')(tower_5a1)
tower_5a1 = SeparableConv2D(40, (7,1), activation='relu', padding='same')(tower_5a1)
tower_5a1 = SeparableConv2D(40, (3,3), activation='relu' , strides=(2,2))(tower_5a1)

tower_5a2 = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_4)
tower_5a2 = SeparableConv2D(24, (3,3), activation='relu' , strides=(2,2))(tower_5a2)

tower_5a3 = MaxPooling2D(pool_size=(3,3), strides=(2,2))(t_4)

t_5 = keras.layers.concatenate([tower_5a1,tower_5a2,tower_5a3], axis = 3)
t_5 = Dropout(0.2)(t_5)
t_5 = BatchNormalization()(t_5,training=True)



out = GlobalAveragePooling2D(data_format='channels_last')(t_5)
out = Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.2))(out)
out = Dropout(0.75)(out)
out = Dense(27, activation='softmax',kernel_regularizer=regularizers.l2(0.2))(out)


model = Model(img_input, out)
print(model.summary())

model.compile(Adam(lr=2.5e-7), loss='categorical_crossentropy', metrics=['accuracy'])

filepath="/home/rishab/temporal2.hdf5"
model.load_weights('/home/rishab/temporal_new.hdf5')
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [EarlyStopping(monitor='val_acc', patience=4, verbose=1),ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')]

#history = model.fit_generator(batches, steps_per_epoch=2483, validation_data=v_batches, validation_steps=719, epochs=18, verbose=1, callbacks=callbacks_list)

K.set_learning_phase(0)
y_pred = model.predict(t_imgs,steps=1)
score = model.evaluate(t_imgs, t_labels, verbose=1)
print(score)
print(model.metrics_names)

model.load_weights('/home/rishab/temporal2.hdf5')

K.set_learning_phase(0)
y_pred = model.predict(t_imgs,steps=1)
score = model.evaluate(t_imgs, t_labels, verbose=1)
print(score)
print(model.metrics_names)

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy no inception')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','valid'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss no inception')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','valid'], loc='upper left')
plt.show()

K.clear_session()
