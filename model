import numpy as np
import keras
from keras import backend as K
from keras.layers import GaussianNoise,Dropout
from keras.models import Sequential
from keras.layers.core import Dense, Flatten
from keras.layers import SeparableConv2D, MaxPooling2D, Activation, Concatenate, AveragePooling2D, Dropout, GlobalAveragePooling2D
from keras.optimizers import Adam, SGD, Adadelta, RMSprop, Nadam
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import *
from keras.layers import Input
from keras.models import Model
import matplotlib.pyplot as plt
import cv2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.image import save_img
import scipy.misc

path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_train'
v_path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_valid'
t_path = '/home/rishab/IIT_M_Internship/ASL/asl_alphabet_test'

batches = ImageDataGenerator().flow_from_directory(path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=32)
v_batches = ImageDataGenerator().flow_from_directory(v_path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=32)
t_batches = ImageDataGenerator().flow_from_directory(t_path, target_size=(224,224), classes=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','space','T','U','V','W','X','Y','Z'], shuffle=True, batch_size=250)
t_imgs,t_labels = next(t_batches)

img_input = Input(shape=(224,224,3))
#image = GaussianNoise(0.01)(img_input)
image = BatchNormalization()(img_input,training=True)
image = SeparableConv2D(16, (1,1), strides=(1,1), activation='relu')(image)
image = SeparableConv2D(16, (5,5), strides=(2,2), activation='relu')(image)
image = MaxPooling2D(pool_size=(3,3), strides=(2,2))(image)
image = SeparableConv2D(32, (3,3), strides=(1,1), activation='relu')(image)
image = BatchNormalization()(image,training=True)


tower_1a = SeparableConv2D(8, (1,1), activation='relu', padding='same')(image)

tower_1b = SeparableConv2D(9, (1,1), activation='relu', padding='same')(image)
tower_1b = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_1b)

tower_1c = SeparableConv2D(2, (1,1), activation='relu', padding='same')(image)
tower_1c = SeparableConv2D(3, (5,1), activation='relu', padding='same')(tower_1c)
tower_1c = SeparableConv2D(3, (1,5), activation='relu', padding='same')(tower_1c)

tower_1d = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(image)
tower_1d = SeparableConv2D(3, (1,1), activation='relu', padding='same')(tower_1d)

t_1 = keras.layers.concatenate([tower_1a,tower_1b,tower_1c,tower_1d], axis = 3)
t_1 = Dropout(0.15)(t_1)
t_1 = BatchNormalization()(t_1,training=True)



tower_2a = SeparableConv2D(12, (1,1), activation='relu', padding='same')(t_1)

tower_2b = SeparableConv2D(12, (1,1), activation='relu', padding='same')(t_1)
tower_2b = SeparableConv2D(18, (3,3), activation='relu', padding='same')(tower_2b)

tower_2c = SeparableConv2D(3, (1,1), activation='relu', padding='same')(t_1)
tower_2c = SeparableConv2D(9, (5,1), activation='relu', padding='same')(tower_2c)
tower_2c = SeparableConv2D(9, (1,5), activation='relu', padding='same')(tower_2c)

tower_2d = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_1)
tower_2d = SeparableConv2D(6, (1,1), activation='relu', padding='same')(tower_2d)

t_2 = keras.layers.concatenate([tower_2a,tower_2b,tower_2c,tower_2d], axis = 3)
t_2 = MaxPooling2D(pool_size=(3,3), strides=(2,2))(t_2)
t_2 = Dropout(0.15)(t_2)
t_2 = BatchNormalization()(t_2,training=True)



tower_3a = SeparableConv2D(12, (1,1), activation='relu', padding='same')(t_2)

tower_3b = SeparableConv2D(8, (1,1), activation='relu', padding='same')(t_2)
tower_3b = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_3b)

tower_3c = SeparableConv2D(8, (1,1), activation='relu', padding='same')(t_2)
tower_3c = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_3c)
tower_3c = SeparableConv2D(12, (3,3), activation='relu', padding='same')(tower_3c)

tower_3d = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_2)
tower_3d = SeparableConv2D(12, (1,1), activation='relu', padding='same')(tower_3d)

t_3 = keras.layers.concatenate([tower_3a,tower_3b,tower_3c,tower_3d], axis = 3)
t_3 = Dropout(0.15)(t_3)
t_3 = BatchNormalization()(t_3,training=True)



tower_4a = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_3)
tower_4a = SeparableConv2D(24, (3,3), activation='relu')(tower_4a)

tower_4b = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_3)
tower_4b = SeparableConv2D(32, (7,1), activation='relu', padding='same')(tower_4b)
tower_4b = SeparableConv2D(40, (1,7), activation='relu', padding='same')(tower_4b)
tower_4b = SeparableConv2D(40, (3,3), activation='relu')(tower_4b)

t_4 = keras.layers.concatenate([tower_4a,tower_4b], axis = 3)
t_4 = Dropout(0.15)(t_4)
t_4 = BatchNormalization()(t_4,training=True)


tower_5a = SeparableConv2D(48, (3,3), activation='relu', strides=(2,2))(t_4)

tower_5b = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_4)
tower_5b = SeparableConv2D(28, (3,3), activation='relu', padding='same')(tower_5b)
tower_5b = SeparableConv2D(32, (3,3), activation='relu' , strides=(2,2))(tower_5b)

tower_5c = MaxPooling2D(pool_size=(3,3), strides=(2,2))(t_4)

t_5 = keras.layers.concatenate([tower_5a,tower_5b,tower_5c], axis = 3)
t_5 = Dropout(0.15)(t_5)
t_5 = BatchNormalization()(t_5,training=True)



tower_6a = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_5)

tower_6b = SeparableConv2D(46, (1,1), activation='relu', padding='same')(t_5)
tower_6b = SeparableConv2D(56, (1,3), activation='relu', padding='same')(tower_6b)
tower_6b = SeparableConv2D(64, (3,1), activation='relu', padding='same')(tower_6b)
tower_6b1 = SeparableConv2D(32, (1,3), activation='relu', padding='same')(tower_6b)
tower_6b2 = SeparableConv2D(32, (3,1), activation='relu', padding='same')(tower_6b)

tower_6c = SeparableConv2D(48, (1,1), activation='relu', padding='same')(t_5)
tower_6c1 = SeparableConv2D(32, (1,3), activation='relu', padding='same')(tower_6c)
tower_6c2 = SeparableConv2D(32, (3,1), activation='relu', padding='same')(tower_6c)

tower_6d = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_5)
tower_6d = SeparableConv2D(32, (1,1), activation='relu', padding='same')(tower_6d)

t_6 = keras.layers.concatenate([tower_6a,tower_6b1,tower_6b2,tower_6c1,tower_6c2,tower_6d], axis = 3)
t_6 = Dropout(0.15)(t_6)
t_6 = BatchNormalization()(t_6,training=True)


"""
tower_7a = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_6)

tower_7b = SeparableConv2D(20, (1,1), activation='relu', padding='same')(t_6)
tower_7b = SeparableConv2D(40, (3,3), activation='relu', padding='same')(tower_7b)

tower_7c = SeparableConv2D(4, (1,1), activation='relu', padding='same')(t_6)
tower_7c = SeparableConv2D(16, (5,1), activation='relu', padding='same')(tower_7c)
tower_7c = SeparableConv2D(16, (1,5), activation='relu', padding='same')(tower_7c)

tower_7d = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_6)
tower_7d = SeparableConv2D(16, (1,1), activation='relu', padding='same')(tower_7d)

t_7 = keras.layers.concatenate([tower_7a,tower_7b,tower_7c,tower_7d], axis = 3)
t_7 = Dropout(0.15)(t_7)
t_7 = BatchNormalization()(t_7,training=True)



tower_8a = SeparableConv2D(48, (1,1), activation='relu', padding='same')(t_7)

tower_8b = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_7)
tower_8b = SeparableConv2D(48, (3,3), activation='relu', padding='same')(tower_8b)

tower_8c = SeparableConv2D(6, (1,1), activation='relu', padding='same')(t_7)
tower_8c = SeparableConv2D(16, (5,1), activation='relu', padding='same')(tower_8c)
tower_8c = SeparableConv2D(16, (1,5), activation='relu', padding='same')(tower_8c)

tower_8d = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(t_7)
tower_8d = SeparableConv2D(16, (1,1), activation='relu', padding='same')(tower_8d)

t_8 = keras.layers.concatenate([tower_8a,tower_8b,tower_8c,tower_8d], axis = 3)
t_8 = Dropout(0.15)(t_8)
t_8 = BatchNormalization()(t_8,training=True)
t_8 = AveragePooling2D(pool_size=(2,2))(t_8)

"""

tower_8a = SeparableConv2D(32, (1,1), activation='relu', padding='same')(t_6)
tower_8a = SeparableConv2D(32, (1,7), activation='relu', padding='same')(tower_8a)
tower_8a = SeparableConv2D(40, (7,1), activation='relu', padding='same')(tower_8a)
tower_8a = SeparableConv2D(40, (3,3), activation='relu' , strides=(2,2))(tower_8a)

tower_8b = SeparableConv2D(24, (1,1), activation='relu', padding='same')(t_7)
tower_8b = SeparableConv2D(24, (3,3), activation='relu' , strides=(2,2))(tower_8b)

tower_8c = MaxPooling2D(pool_size=(3,3), strides=(2,2))(t_7)

t_8 = keras.layers.concatenate([tower_8a,tower_8b,tower_8c], axis = 3)
t_8 = Dropout(0.15)(t_8)
t_8 = BatchNormalization()(t_8,training=True)



out = GlobalAveragePooling2D(data_format='channels_last')(t_9)
out = Dense(100, activation='relu')(out)
out = Dropout(0.70)(out)
out = Dense(27, activation='softmax')(out)


model = Model(img_input, out)
print(model.summary())

model.compile(Adam(lr=2.5e-4), loss='categorical_crossentropy', metrics=['accuracy'])



filepath="/home/rishab/model.hdf5"
#model.load_weights('/home/rishab/temporal.hdf5')
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [EarlyStopping(monitor='val_acc', patience=4, verbose=1),ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')]

history = model.fit_generator(batches, steps_per_epoch=2483, validation_data=v_batches, validation_steps=719, epochs=5, verbose=1, callbacks=callbacks_list)

K.set_learning_phase(0)
y_pred = model.predict(t_imgs,steps=1)
score = model.evaluate(t_imgs, t_labels, verbose=1)
print(score)
print(model.metrics_names)

K.set_learning_phase(1)
y_pred = model.predict(t_imgs,steps=1)
score = model.evaluate(t_imgs, t_labels, verbose=1)
print(score)
print(model.metrics_names)



plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy no inception')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','valid'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss no inception')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','valid'], loc='upper left')
plt.show()


K.clear_session()
